{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run hunnae_news_scraper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20121218\n",
    "end = 20121218\n",
    "query = '18대 대선'\n",
    "press_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt') :\n",
    "    os.remove(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt')\n",
    "\n",
    "news_links, title_list, date_list, content_list, query_list, press_list, category_list, commentcount_list = ([] for i in range(8))\n",
    "date_list = [datetime.strptime(str(end),'%Y%m%d') - timedelta(days=x) \n",
    "         for x in range(0, (datetime.strptime(str(end),'%Y%m%d')-datetime.strptime(str(start),'%Y%m%d')).days+1)]\n",
    "session = requests.Session()\n",
    "header = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)\\AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "          \"Accept\":\"text/html,application/xhtml+xml,application/xml;\\q=0.9,imgwebp,*/*;q=0.8\"}\n",
    "url = daum_url(start, end, query, press_name)\n",
    "req = session.get(url, headers=header)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "total_num_article = int(''.join(re.findall('\\d+',re.search(r'/ (.*?)건',soup.find('span', attrs={'class':'txt_info'}).text).group(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_date = date_list[0]\n",
    "new_url = daum_url(int(each_date.strftime('%Y%m%d')), int(each_date.strftime('%Y%m%d')), query, press_name)\n",
    "new_req = session.get(new_url, headers=header)\n",
    "new_soup = BeautifulSoup(new_req.text, 'html.parser')\n",
    "num_article = int(''.join(re.findall('\\d+',re.search(r'/ (.*?)건',new_soup.find('span', attrs={'class':'txt_info'}).text).group(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1820"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_date in date_list :\n",
    "    time.sleep(.25)\n",
    "    new_url = daum_url(int(each_date.strftime('%Y%m%d')), int(each_date.strftime('%Y%m%d')), query, press_name)\n",
    "    new_req = session.get(new_url, headers=header)\n",
    "    new_soup = BeautifulSoup(new_req.text, 'html.parser')\n",
    "    num_article = int(''.join(re.findall('\\d+',re.search(r'/ (.*?)건',new_soup.find('span', attrs={'class':'txt_info'}).text).group(1))))\n",
    "\n",
    "    if num_article == 0 :\n",
    "        pass\n",
    "    else :\n",
    "        for each_page in range(1,(num_article//10)+2) :\n",
    "            new_soup2 = BeautifulSoup(session.get(\n",
    "                daum_url(int(each_date.strftime('%Y%m%d')), int(each_date.strftime('%Y%m%d')), query, press_name, each_page), headers=header).text, 'html.parser')\n",
    "            for each_link in new_soup2.find_all('div', attrs={'class':'wrap_tit mg_tit'}) :\n",
    "                news_links.append(each_link.a['href'][:-4])\n",
    "                query_list.append(query)\n",
    "                press_list.append(press_name)\n",
    "                try :\n",
    "                    title, date, content, category, comment_count = daumlinknews_scraper(each_link.a['href'][:-4])\n",
    "                    with open(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt','a') as f :\n",
    "                        f.writelines(each_link.a['href'][:-4]+'\\n')\n",
    "                except :\n",
    "                    title, date, content, category, comment_count = [None for i in range(5)]\n",
    "                    with open(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt','a') as f :\n",
    "                        f.writelines('[ERROR] ' + each_link.a['href'][:-4] + '\\n')\n",
    "                title_list.append(title)\n",
    "                date_list.append(date)\n",
    "                content_list.append(content)\n",
    "                category_list.append(category)\n",
    "                commentcount_list.append(comment_count)\n",
    "\n",
    "                time.sleep(.25)\n",
    "                clear_output(wait=True)\n",
    "                print(len(news_links),'/',total_num_article)\n",
    "\n",
    "print(len(news_links),'/',total_num_article, 'COMPLETE!')\n",
    "\n",
    "final_df = pd.DataFrame(zip(news_links, query_list, press_list, title_list, \n",
    "                        date_list, category_list, content_list, commentcount_list),\n",
    "     columns=['url', 'query', 'press', 'title', 'date', 'category', 'content', 'commentcount'])\n",
    "final_df = final_df.dropna(subset=['title']).drop_duplicates(subset=['url', 'title']).reset_index().drop('index', axis=1)\n",
    "final_df['commentcount'] = final_df['commentcount'].astype(int)\n",
    "final_df.to_csv(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.csv')\n",
    "\n",
    "with open(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.p', 'wb') as f :\n",
    "pickle.dump(final_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hunnae_scraper(start, end, query, press_name) :\n",
    "    if os.path.exists(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt') :\n",
    "        os.remove(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt')\n",
    "        \n",
    "    news_links, title_list, date_list, content_list, query_list, press_list, category_list, commentcount_list = ([] for i in range(8))\n",
    "    date_list = [datetime.strptime(str(end),'%Y%m%d') - timedelta(days=x) \n",
    "             for x in range(0, (datetime.strptime(str(end),'%Y%m%d')-datetime.strptime(str(start),'%Y%m%d')).days+1)]\n",
    "    session = requests.Session()\n",
    "    header = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)\\AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "              \"Accept\":\"text/html,application/xhtml+xml,application/xml;\\q=0.9,imgwebp,*/*;q=0.8\"}\n",
    "    url = daum_url(start, end, query, press_name)\n",
    "    req = session.get(url, headers=header)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    total_num_article = int(''.join(re.findall('\\d+',re.search(r'/ (.*?)건',soup.find('span', attrs={'class':'txt_info'}).text).group(1))))\n",
    "    \n",
    "    if total_num_article <= 800 :\n",
    "        for each in range(1,(total_num_article//10)+2) :\n",
    "            new_soup = BeautifulSoup(session.get(daum_url(start, end, query, press_name, each), headers=header).text, 'html.parser')\n",
    "            for each_link in new_soup.find_all('div', attrs={'class':'wrap_tit mg_tit'}) :\n",
    "                news_links.append(each_link.a['href'][:-4])\n",
    "                query_list.append(query)\n",
    "                press_list.append(press_name)\n",
    "                try :\n",
    "                    title, date, content, category, comment_count = daumlinknews_scraper(each_link.a['href'][:-4])\n",
    "                    with open(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt','a') as f :\n",
    "                        f.writelines(each_link.a['href'][:-4]+'\\n')\n",
    "                except :\n",
    "                    title, date, content, category, comment_count = [None for i in range(5)]\n",
    "                    with open(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt','a') as f :\n",
    "                        f.writelines('[ERROR] ' + each_link.a['href'][:-4] + '\\n')\n",
    "                title_list.append(title)\n",
    "                date_list.append(date)\n",
    "                content_list.append(content)\n",
    "                category_list.append(category)\n",
    "                commentcount_list.append(comment_count)\n",
    "                \n",
    "                time.sleep(.25)\n",
    "                clear_output(wait=True)\n",
    "                print(len(news_links),'/',total_num_article)\n",
    "\n",
    "    else :\n",
    "        for each_date in date_list :\n",
    "            time.sleep(.25)\n",
    "            new_url = daum_url(int(each_date.strftime('%Y%m%d')), int(each_date.strftime('%Y%m%d')), query, press_name)\n",
    "            new_req = session.get(new_url, headers=header)\n",
    "            new_soup = BeautifulSoup(new_req.text, 'html.parser')\n",
    "            num_article = int(''.join(re.findall('\\d+',re.search(r'/ (.*?)건',new_soup.find('span', attrs={'class':'txt_info'}).text).group(1))))\n",
    "\n",
    "            if num_article == 0 :\n",
    "                pass\n",
    "            else :\n",
    "                for each_page in range(1,(num_article//10)+2) :\n",
    "                    new_soup2 = BeautifulSoup(session.get(\n",
    "                        daum_url(int(each_date.strftime('%Y%m%d')), int(each_date.strftime('%Y%m%d')), query, press_name, each_page), headers=header).text, 'html.parser')\n",
    "                    for each_link in new_soup2.find_all('div', attrs={'class':'wrap_tit mg_tit'}) :\n",
    "                        news_links.append(each_link.a['href'][:-4])\n",
    "                        query_list.append(query)\n",
    "                        press_list.append(press_name)\n",
    "                        try :\n",
    "                            title, date, content, category, comment_count = daumlinknews_scraper(each_link.a['href'][:-4])\n",
    "                            with open(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt','a') as f :\n",
    "                                f.writelines(each_link.a['href'][:-4]+'\\n')\n",
    "                        except :\n",
    "                            title, date, content, category, comment_count = [None for i in range(5)]\n",
    "                            with open(str(start)+'_'+str(end)+'_'+query+'_'+str(press_name)+'.txt','a') as f :\n",
    "                                f.writelines('[ERROR] ' + each_link.a['href'][:-4] + '\\n')\n",
    "                        title_list.append(title)\n",
    "                        date_list.append(date)\n",
    "                        content_list.append(content)\n",
    "                        category_list.append(category)\n",
    "                        commentcount_list.append(comment_count)\n",
    "\n",
    "                        time.sleep(.25)\n",
    "                        clear_output(wait=True)\n",
    "                        print(len(news_links),'/',total_num_article)\n",
    "\n",
    "    print(len(news_links),'/',total_num_article, 'COMPLETE!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
