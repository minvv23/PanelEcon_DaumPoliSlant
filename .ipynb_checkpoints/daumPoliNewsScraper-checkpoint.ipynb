{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from itertools import chain\n",
    "from datetime import date, datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random, hickle, sys, math, pickle, json, requests, urllib, urllib.request, urllib.parse, csv, re, time, os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "basic_header = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)\\AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "          \"Accept\":\"text/html,application/xhtml+xml,application/xml;\\q=0.9,imgwebp,*/*;q=0.8\"}\n",
    "reply_header = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)\\AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "          \"Authorization\":\"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzY29wZSI6WyJST0xFX0NMSUVOVCJdLCJleHAiOjE1NTYyNTM4MzksImF1dGhvcml0aWVzIjpbIlJPTEVfQ0xJRU5UIl0sImp0aSI6IjM3OGQyMGVkLWQ1YWEtNDRjNS05ZGQ1LWJiY2M0Y2FkZGNiMCIsImNsaWVudF9pZCI6IjI2QlhBdktueTVXRjVaMDlscjVrNzdZOCJ9.vsaEptsgS0PiSMpxlBImlMYU4BfdlVc-7RGxvWKe7B0\"}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "#options.add_argument('headless') #headless 설정은 이 코드로!\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daumlinknews_scraper(url) :\n",
    "    basic_header = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)\\AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "                    \"Accept\":\"text/html,application/xhtml+xml,application/xml;\\q=0.9,imgwebp,*/*;q=0.8\"}\n",
    "    reply_header = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)\\AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "                    \"Authorization\":\"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJncmFudF90eXBlIjoiYWxleF9jcmVkZW50aWFscyIsInNjb3BlIjpbXSwiZXhwIjoxNTY2NTExMjAxLCJhdXRob3JpdGllcyI6WyJST0xFX0NMSUVOVCJdLCJqdGkiOiJiZDY4ZGM5ZS03YTMwLTQ0OTItOGJmOS03ZTI3MTUwZjU4MjQiLCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgifQ.-DGIRFH9y1iXF8-5SbDszEdqN5PlHIoeQ1jvtAqAEaw\"}\n",
    "    pre_replyurl = 'https://comment.daum.net/apis/v1/posts/@%s/comments?parentId=0&offset=0&limit=1000&sort=CHRONOLOGICAL'\n",
    "    pre_subreplyurl = 'https://comment.daum.net/apis/v1/comments/%d/children?offset=0&limit=%d&sort=CHRONOLOGICAL'\n",
    "    soup = BeautifulSoup(requests.Session().get(url, headers=basic_header).text, 'html.parser')\n",
    "    \n",
    "    try :\n",
    "        if soup.find('p', attrs={'class':'desc_empty'}) == None :\n",
    "            postKey = soup.find('meta', attrs={'property':'dg:uoc:uid'})['content']\n",
    "            category = soup.find('h2', attrs={'id':'kakaoBody'}).text\n",
    "            title = soup.find('h3', attrs={'class':'tit_view'}).text\n",
    "            content = ' '.join(' '.join([each.text for each in soup.find_all(['p','div'], attrs={'dmcf-ptype':'general'})]).split())\n",
    "            press = soup.find('em', attrs={'class':'info_cp'}).a.img['alt']\n",
    "            reporter = None\n",
    "            for each_info in soup.find_all('span', attrs={'class':'txt_info'}) :\n",
    "                if '입력 201' in each_info.text :\n",
    "                    createdAt = each_info.text[3:]\n",
    "                if '입력 201' not in each_info.text and '수정 2019' not in each_info.text :\n",
    "                    reporter = each_info.text\n",
    "            reply = list(requests.Session().get(pre_replyurl %postKey, headers=reply_header).json())\n",
    "            for each_reply in reply :\n",
    "                if each_reply['childCount']!=0 :\n",
    "                    each_reply['subreply'] = list(requests.Session().get(pre_subreplyurl %(each_reply['id'], each_reply['childCount']), headers=basic_header).json())\n",
    "                else :\n",
    "                    each_reply['subreply'] = None\n",
    "    \n",
    "        return title, content, postKey, category, createdAt, press, reporter, reply\n",
    "    except :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:/Users/Hunnae/Downloads/chromedriver', options=options)\n",
    "issue_url = 'https://media.daum.net/issue/politics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_topics = []\n",
    "for pagenum in range(1,4) :\n",
    "    driver.get(issue_url + str(pagenum))\n",
    "    for each in BeautifulSoup(driver.page_source, 'html.parser').find_all('strong', attrs={'class':'tit_thumb'}) :\n",
    "        issue_topics.append('https://media.daum.net'+each.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [08:30<00:00,  5.09s/it]\n"
     ]
    }
   ],
   "source": [
    "topic_list, article_url_list = ([] for i in range(2))\n",
    "for each_topic in tqdm(issue_topics) :\n",
    "    driver.get(each_topic)\n",
    "    while True :\n",
    "        try :\n",
    "            time.sleep(1)\n",
    "            driver.find_element_by_xpath('//*[@id=\"mArticle\"]/a').click()\n",
    "        except :\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    topic = soup.find('h3', attrs={'class':'tit_head'}).text\n",
    "    article_urls = [each.a['href'] for each in soup.find_all('strong', attrs={'class':'tit_thumb'})] + [each.a['href'] for each in soup.find_all('div', attrs={'class':'item_relate'})]\n",
    "    \n",
    "    topic_list.append(topic)\n",
    "    article_url_list.append(article_urls)\n",
    "\n",
    "with open('issueTopicLinks.p', 'wb') as p :\n",
    "    pickle.dump(list(zip(topic_list, article_url_list)), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('issueTopicLinks.p', 'rb') as p :\n",
    "    issuetopic_links = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "issuetopic_df = pd.DataFrame(chain.from_iterable([list(zip(np.repeat(topic, len(url_chunks)), url_chunks)) for topic, url_chunks in issuetopic_links]),\n",
    "                             columns=['topic', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4859 / 4859\n",
      "COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "dauminfo_list = []\n",
    "for each_url in issuetopic_df['url'] :\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(.3)\n",
    "    dauminfo_list.append(daumlinknews_scraper(each_url))\n",
    "    print(len(dauminfo_list), '/', len(issuetopic_df['url']))\n",
    "print('COMPLETE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('totalDaumInfo.p', 'wb') as p :\n",
    "    pickle.dump(dauminfo_list, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dauminfo_df = pd.DataFrame(dauminfo_list,\n",
    "                           columns=['title', 'content', 'postKey', 'category', 'date', 'press', 'reporter', 'reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_df.p', 'wb') as f :\n",
    "    pickle.dump(pd.concat([issuetopic_df, dauminfo_df], axis =1), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_daum_df.p', 'rb') as f :\n",
    "    total_daum_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "directory_address = \n",
    "\n",
    "for article in total_daum_df['reply'] :\n",
    "    try :\n",
    "        for each_review in article :\n",
    "            if each_review['childCount']>0 :\n",
    "                try :\n",
    "                    imagelink = each_review['subreply'][0]['post']['icon']\n",
    "                except :\n",
    "                    imagelink = None\n",
    "            if imagelink!=None :\n",
    "                break\n",
    "    except :\n",
    "        imagelink = None\n",
    "    image_list.append(imagelink)\n",
    "image_list = pd.Series(image_list).rename('thumbnail_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_daum.p', 'wb') as f :\n",
    "    pickle.dump(total_daum, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_daum.p', 'rb') as f :\n",
    "    total_daum = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190725085154329'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_daum['postKey'][1993]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://img1.daumcdn.net/thumb/S1200x630/?fname=https://t1.daumcdn.net/news/201907/18/imbc/20190718115302103hpww.jpeg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_daum['thumbnail_url'][1993]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4846 / 4847\n"
     ]
    }
   ],
   "source": [
    "counter = 1992\n",
    "for idx in range(1993, len(total_daum)) :\n",
    "    clear_output(wait=True)\n",
    "    counter += 1\n",
    "    time.sleep(.2)\n",
    "    try :\n",
    "        urllib.request.urlretrieve(str(total_daum['thumbnail_url'][idx]), \"./thumbnail_images/\"+str(total_daum['postKey'][idx])+'.jpg')\n",
    "    except :\n",
    "        pass\n",
    "    print(counter, '/', len(total_daum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
