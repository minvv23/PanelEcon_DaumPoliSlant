{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from selenium import webdriver\n",
    "from datetime import date, datetime, timedelta\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import sys, math, pickle, json, requests, urllib, urllib.request, urllib.parse, csv, re, time, os, numpy as np, pandas as pd\n",
    "\n",
    "total_newslinks = pd.Series.tolist(pd.read_csv('total_daum_links.csv')['0'])[:1550]\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless') #headless 설정은 이 코드로!\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:/Users/Hunnae/Downloads/chromedriver', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list, createdAt_list, num_reply_list, press_list, category_list, reporter_list, title_list, content_list, reply_list = ([] for i in range(9))\n",
    "\n",
    "for each_article_http in total_newslinks[370:600] :\n",
    "    url = each_article_http\n",
    "    driver.get(url)\n",
    "    time.sleep(.5)\n",
    "    \n",
    "    try :\n",
    "        driver.execute_script(\"\"\"\n",
    "        var element = arguments[0];\n",
    "        element.parentNode.removeChild(element);\n",
    "        \"\"\", driver.find_element_by_class_name('vod_cluster2'))\n",
    "    except :\n",
    "        pass\n",
    "    \n",
    "    #과거순으로 정렬하기\n",
    "    for k in range(2):\n",
    "        try :\n",
    "            driver.find_element_by_class_name('point_box')\n",
    "            driver.find_element_by_xpath('//*[@id=\"alex-area\"]/div/div/div/div[4]/ul[1]/li[3]/button').click()\n",
    "        except :\n",
    "            driver.find_element_by_xpath('//*[@id=\"alex-area\"]/div/div/div/div[3]/ul[1]/li[3]/button').click()   \n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    if len(soup.find_all('span', attrs={'class':'txt_info'}))<=2 :\n",
    "        createdAt = soup.find_all('span', attrs={'class':'txt_info'})[0].text[3:]\n",
    "        reporter = None\n",
    "    else :\n",
    "        createdAt = soup.find_all('span', attrs={'class':'txt_info'})[1].text[3:]\n",
    "        reporter = soup.find_all('span', attrs={'class':'txt_info'})[0].text\n",
    "    press = soup.find('em', attrs={'class':'info_cp'}).a.img['alt']\n",
    "    category = soup.find('h2', attrs={'id':'kakaoBody'}).text\n",
    "    title = soup.find('h3', attrs={'class':'tit_view'}).text\n",
    "    content = ' '.join(''.join([each.text for each in soup.find_all(['p','div'], attrs={'dmcf-ptype':'general'})]).split())    \n",
    "    num_reply = int(soup.find('em', attrs={'class':'num_count'}).text)\n",
    "        \n",
    "    #기사 메인 페이지에서 스크롤 끝까지 내리기\n",
    "    for i in range(math.ceil((num_reply-3)/10)+2) :\n",
    "        try :\n",
    "            time.sleep(.1)\n",
    "            driver.find_element_by_xpath('//*[@id=\"alex-area\"]/div/div/div/div[3]/div[1]/a').click()\n",
    "        except :\n",
    "            break\n",
    "    \n",
    "    reply_links = BeautifulSoup(driver.page_source, 'lxml').find_all('div',attrs={'class':'cmt_info'})\n",
    "    reply_sublist = [{} for i in range(len(reply_links))]\n",
    "\n",
    "    for each_index in range(len(reply_links)) :\n",
    "        try:\n",
    "            if reply_links[each_index].p != None :\n",
    "                reply_sublist[each_index]['content'] = ' '.join(reply_links[each_index].p.text.split())\n",
    "            else :\n",
    "                reply_sublist[each_index]['content'] = None\n",
    "                \n",
    "            reply_sublist[each_index]['displayName'] = reply_links[each_index].a.text\n",
    "            reply_sublist[each_index]['userId'] = reply_links[each_index].a['data-userid']\n",
    "            reply_sublist[each_index]['id'] = re.findall('\\$(.*).0.0', reply_links[each_index].a['data-reactid'])[0]\n",
    "            reply_sublist[each_index]['parentId'] = 0\n",
    "            reply_sublist[each_index]['postKey'] = re.findall(r'\\d+', url)[0]\n",
    "            reply_sublist[each_index]['likeCount'] = int(reply_links[each_index].find('button', attrs={'class':'#like'}).find('span', attrs={'class':'num_txt'}).text)\n",
    "            reply_sublist[each_index]['dislikeCount'] = int(reply_links[each_index].find('button', attrs={'class':'#dislike'}).find('span', attrs={'class':'num_txt'}).text)\n",
    "            reply_sublist[each_index]['createdAt'] = reply_links[each_index].find('span', attrs={'class':'txt_date'}).text\n",
    "\n",
    "            if reply_links[each_index].find('span', attrs={'class':'img_cmt img_kakao'})!=None :\n",
    "                reply_sublist[each_index]['providerId'] = 'KAKAO'\n",
    "            else :\n",
    "                reply_sublist[each_index]['providerId'] = 'DAUM'\n",
    "\n",
    "            if reply_links[each_index].find('button', attrs={'class':'reply_count'}).find('span',attrs={'class':'num_txt'})!=None :\n",
    "                reply_sublist[each_index]['subreplynum'] = int(reply_links[each_index].find('button', attrs={'class':'reply_count'}).find('span',attrs={'class':'num_txt'}).text)\n",
    "            else :\n",
    "                reply_sublist[each_index]['subreplynum'] = 0\n",
    "\n",
    "            if reply_sublist[each_index]['subreplynum'] > 0 :\n",
    "                reply_sublist[each_index]['subreplyinfo'] = json.loads(\n",
    "                    re.search(r'\\[(.*)\\]',str(BeautifulSoup(requests.Session().get('https://comment.daum.net/apis/v1/comments/'\n",
    "                                     + reply_sublist[each_index]['id']\n",
    "                                     + '/children?offset=0&limit='\n",
    "                                     + str(reply_sublist[each_index]['subreplynum'])\n",
    "                                     + '&sort=CHRONOLOGICAL').text, 'html.parser')))[0])\n",
    "            else:\n",
    "                reply_sublist[each_index]['subreplyinfo'] = None\n",
    "        except KeyError :\n",
    "            pass\n",
    "        \n",
    "    url_list.append(url)\n",
    "    createdAt_list.append(createdAt)\n",
    "    num_reply_list.append(num_reply)\n",
    "    press_list.append(press)\n",
    "    category_list.append(category)\n",
    "    reporter_list.append(reporter)\n",
    "    title_list.append(title)\n",
    "    content_list.append(content)\n",
    "    reply_list.append(reply_sublist)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(len(reporter_list),'/',len(total_newslinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe = pd.DataFrame(\n",
    "    {'url': url_list,\n",
    "     'createdAt': createdAt_list,\n",
    "     'num_reply': num_reply_list,\n",
    "     'press':press_list,\n",
    "     'category':category_list,\n",
    "     'reporter':reporter_list,\n",
    "     'title':title_list,\n",
    "     'content':content_list,\n",
    "     'reply':reply_list\n",
    "    })\n",
    "\n",
    "with open('370-506_DAUM_total_articles_info.p', 'wb') as f :\n",
    "    pickle.dump(total_dataframe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_dataframe.p', 'rb') as f :\n",
    "    test2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
